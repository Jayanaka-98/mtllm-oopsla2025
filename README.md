# MTLLM: OOPSLA 2025 Artifact

**Meaning-Typed Programming: Language Abstraction and Runtime for Model-Integrated Applications**

[![Paper](https://img.shields.io/badge/Paper-OOPSLA%202025-blue)](https://doi.org/10.1145/placeholder)
[![Artifact](https://img.shields.io/badge/Artifact-In_Production-green)](https://doi.org/10.1145/placeholder)

## Overview

This artifact accompanies the OOPSLA 2025 paper "Meaning-Typed Programming: Language Abstraction and Runtime for Model-Integrated Applications". It provides a complete implementation of **MTLLM**, a novel programming language abstraction that enables type-safe integration of Large Language Models (LLMs) into traditional programming workflows.

> **The Meaning-Typed Programming (MTP) paradigm is implemented in the Jaseci ecosystem as MTLLM plugin to the Jac programming language. Whats being reffered to as the 'MTP' implementation in the paper is this MTLLM plugin.**

**Key Innovation**: MTLLM bridges the gap between the structured world of programming languages and the unstructured outputs of LLMs through a type system that captures both structural types and semantic meaning, enabling compile-time guarantees for AI-powered functions.

### Primary Contributions

1. **Type-Safe LLM Integration**: Compile-time type checking for LLM-powered functions with runtime output validation
2. **Automatic Output Transformation**: Runtime system that converts unstructured LLM outputs into typed programming language objects
3. **Semantic Type System**: Type annotations that capture both structural types (`int`, `str`) and semantic meaning for precise LLM guidance
4. **Language-Integrated AI**: Native `by llm()` syntax in the Jac programming language for seamless AI integration

### Artifact Contents

This repository contains:

- **Complete MTLLM implementation** for the Jac programming language (version 0.3.8)
- **Comprehensive benchmark suite** with 12 tasks comparing MTLLM against DSPy and LMQL baselines
- **Evaluation scripts** for reproducing all experimental results from the paper
- **Documentation and examples** demonstrating all key features
- **Docker environment** for reproducible evaluation

The implementation is based on the open-source [Jaseci ecosystem](https://www.jac-lang.org/learn/jac-mtllm/with_llm/) and represents the exact version used for paper evaluation.

## Getting Started

### Prerequisites

- **Python 3.12+**: Required for the Jac language runtime
- **OpenAI API Key**: Required for evaluation benchmarks using GPT models
- **Operating System**: Linux or macOS (Windows not currently supported)
- **Docker** (optional): For containerized evaluation environment

### Quick Start Options

#### Option 1: Direct Installation

```bash
# Clone the repository with submodules
git clone --recurse-submodules https://github.com/Jayanaka-98/mtllm-oopsla2025.git
cd mtllm-oopsla2025

# Install MTLLM with all required dependencies
pip install "mtllm[openai,ollama,tools]==0.3.8"

# Install evaluation dependencies
pip install -r eval/requirements.txt

# Set up your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"

# Optional: Install Ollama for local model evaluation
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Option 2: Docker Environment (Recommended)

For a fully reproducible environment:

```bash
# Clone the repository
git clone --recurse-submodules https://github.com/Jayanaka-98/mtllm-oopsla2025.git
cd mtllm-oopsla2025

# Build and start the Docker container
chmod +x setup.bash
./setup.bash

# Inside the container, set your API key
export OPENAI_API_KEY="your-api-key-here"
```

### Verification

Test your installation by running a simple MTLLM example:

```bash
# Create a test file
cat > test.jac << 'EOF'
import from mtllm.llms {OpenAI}

glob llm = OpenAI(model_name="gpt-4o");

def greet(name: str) -> str by llm();

with entry {
    print(greet("OOPSLA reviewers"));
}
EOF

# Run the test
jac run test.jac
```

If successful, you should see a greeting message generated by the LLM.

## Core Features and Examples

The following examples demonstrate the three main usage patterns of MTLLM, corresponding to Figures 8(a), 8(b), and 8(c) in the paper.

### 1. Type-Safe LLM Functions

MTLLM functions allow you to define function signatures with traditional type annotations while delegating implementation to an LLM. The runtime ensures type safety by validating and converting LLM outputs.

**Example: Basic function with type enforcement**
```jac
import from mtllm.llms {OpenAI}

# Initialize the LLM
glob llm = OpenAI(model_name="gpt-4o");

# Define a type-safe LLM function
def calculate_age(cur_year: int, dob: str) -> int by llm();

with entry {
    age = calculate_age(cur_year=2025, dob="1998");
    print(f"Age: {age}");  # Output is guaranteed to be an integer
}
```

**Run**: `jac run func.jac`

### 2. LLM-Powered Object Construction

MTLLM can generate object fields automatically while maintaining type constraints, enabling AI-driven object initialization with structural guarantees.

**Example: Automatic field generation**
```jac
import from mtllm.llms {OpenAI}

glob llm = OpenAI(model_name="gpt-4o");

obj Person {
    has name: str;
    has dob: str;
}

with entry {
    # LLM fills in missing field based on partial information
    einstein = Person(name="Einstein" by llm());
    print(f"{einstein.name} was born on {einstein.dob}");
}
```

**Run**: `jac run object.jac`

### 3. LLM-Enhanced Object Methods

Methods can leverage LLM capabilities while accessing object state, enabling context-aware AI computations with type safety.

**Example: Context-aware method with object state access**
```jac
import from mtllm.llms {OpenAI}

glob llm = OpenAI(model_name="gpt-4o");

obj Person {
    has name: str;
    has dob: str;

    # Method uses object state (self) for computation
    def calculate_age(cur_year: int) -> int by llm(incl_info=(self), temperature=0.7);
}

with entry {
    einstein = Person(name="Einstein", dob="March 14, 1879");
    print(f"Einstein's age in 2024: {einstein.calculate_age(2024)}");
}
```

**Run**: `jac run method.jac`

### Advanced Features

- **Multiple LLM Support**: OpenAI GPT, Anthropic Claude, local models via Ollama
- **Type Coercion**: Automatic parsing and validation of complex types (lists, objects, enums)
- **Error Recovery**: Robust handling of malformed LLM outputs with retry mechanisms
- **Native Agentic Support**: MTLLM supports ReAct to be used to build agentic applications
- **Vision Model Support**: MTLLM can inference with muilti-modal models which can take images and videos as inputs.

> **ðŸ“– Complete Documentation**: [MTLLM User Guide](https://www.jac-lang.org/learn/jac-mtllm)

## Evaluation and Benchmarks

This artifact includes a comprehensive evaluation suite that reproduces all experimental results from the paper. The benchmarks compare MTLLM against two state-of-the-art frameworks: DSPy and LMQL.

### Benchmark Tasks

The evaluation covers 12 diverse tasks across different domains:

| Category | Task | Description |
|----------|------|-------------|
| **Text Processing** | `translation` | Multi-language text translation |
| | `text_to_type` | Converting unstructured text to typed objects |
| **Reasoning** | `mcq_reason` | Multiple-choice question reasoning |
| | `math_problem` | Mathematical word problem solving |
| | `odd_word_out` | Pattern recognition and categorization |
| **Content Generation** | `joke_gen` | Creative content generation |
| | `essay_reviewer` | Academic text analysis |
| | `expert_answer` | Domain-specific question answering |
| **Applications** | `taskman` | Task management and scheduling |
| | `rpg_level_gen` | Game content generation |
| | `personality_finder` | Personality analysis |
| | `wikipedia` | Information extraction and summarization |

### Running the Complete Evaluation

```bash
# Run all benchmarks (requires OpenAI API key)
cd eval
python eval.py

# Run specific benchmark categories
python eval.py --tasks translation,text_to_type,mcq_reason

# Generate summary statistics
python overall_accuracy.py
```

### Expected Results

The evaluation will generate:
- `benchmark_detailed_results_[timestamp].csv`: Detailed per-task results
- `benchmark_summary_results_[timestamp].csv`: Aggregated performance metrics
- Console output showing real-time progress and accuracy scores

### Performance Metrics

The evaluation measures:
- **Accuracy**: Task-specific correctness metrics
- **Token Usage**: Total tokens consumed per task
- **Runtime**: Execution time per benchmark
- **Cost**: Estimated API costs (USD)

<!-- ## Claims Validation

The paper makes four key claims that this artifact validates:

### Claim 1: Development Complexity Reduction
*MTLLM reduces development complexity for model-integrated applications*

**Evidence**: Compare MTLLM implementations with DSPy/LMQL baselines in `/benchmarks/` directory. MTLLM consistently requires fewer lines of code and less boilerplate.

### Claim 2: Competitive Accuracy  
*MTLLM achieves similar or better accuracy than baseline frameworks*

**Evidence**: Run evaluation suite to reproduce accuracy results from Table 2 in the paper.

### Claim 3: Efficient Resource Usage
*MTLLM demonstrates similar or lower token usage, cost, and runtime compared to baselines*

**Evidence**: Resource usage metrics are captured during evaluation and match paper results.

### Claim 4: Resilience to Coding Practices
*MTLLM demonstrates resilience to suboptimal coding practices*

**Evidence**: Robustness tests show MTLLM maintains performance across different implementation styles. -->

## Interactive Demo

Experience MTLLM with the included RPG game that uses LLM-powered procedural level generation:

```bash
# Install game dependencies
pip install pygame

# Run the interactive RPG demo
cd jaseci/jac/examples/rpg_game/jac_impl/jac_impl_6
jac run main.jac
```

This demonstrates real-world application of MTLLM for dynamic content generation in an interactive environment.

## Repository Structure

```
mtllm-oopsla2025/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ Dockerfile                   # Docker environment setup
â”œâ”€â”€ setup.bash                   # Automated setup script
â”œâ”€â”€ benchmarks/                  # Evaluation benchmarks
â”‚   â”œâ”€â”€ translation/            # Translation task implementations
â”‚   â”œâ”€â”€ text_to_type/           # Text-to-type conversion tasks
â”‚   â”œâ”€â”€ mcq_reason/             # Multiple choice reasoning
â”‚   â”œâ”€â”€ math_problem/           # Mathematical problem solving
â”‚   â”œâ”€â”€ joke_gen/               # Content generation tasks
â”‚   â”œâ”€â”€ essay_reviewer/         # Text analysis tasks
â”‚   â”œâ”€â”€ expert_answer/          # Domain-specific QA
â”‚   â”œâ”€â”€ taskman/                # Task management
â”‚   â”œâ”€â”€ rpg_level_gen/          # Game content generation
â”‚   â”œâ”€â”€ personality_finder/     # Personality analysis
â”‚   â”œâ”€â”€ odd_word_out/           # Pattern recognition
â”‚   â”œâ”€â”€ wikipedia/              # Information extraction
â”‚   â””â”€â”€ template/               # Template for new benchmarks
â”œâ”€â”€ eval/                       # Evaluation scripts and results
â”‚   â”œâ”€â”€ eval.py                 # Main evaluation runner
â”‚   â”œâ”€â”€ overall_accuracy.py     # Results aggregation
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ local_cache/            # Cached compilation artifacts
â””â”€â”€ jaseci/                     # Core Jaseci ecosystem
    â”œâ”€â”€ jac/                    # Jac language implementation
    â”œâ”€â”€ jac-mtllm/              # MTLLM plugin source
    â”œâ”€â”€ jac-cloud/              # Cloud deployment tools
    â””â”€â”€ scripts/                # Utility scripts
```

Each benchmark directory contains three implementations:
- `*_mtllm.jac`: MTLLM implementation
- `*_dspy.py`: DSPy baseline implementation  
- `*_lmql.py`: LMQL baseline implementation

## Troubleshooting

### Common Issues

**Python Version Error**
```bash
ERROR: Python 3.12+ required
```
*Solution*: Upgrade Python or use the Docker environment.

**API Key Error**
```bash
openai.AuthenticationError: Invalid API key
```
*Solution*: Verify your OpenAI API key is set correctly:
```bash
echo $OPENAI_API_KEY  # Should display your key
export OPENAI_API_KEY="your-actual-key-here"
```

**Package Installation Error**
```bash
ERROR: Could not find a version that satisfies mtllm
```
*Solution*: Ensure you're using Python 3.12+ and run:
```bash
pip install --upgrade pip
pip install "mtllm[openai,ollama,tools]==0.3.8"
```

**Ollama Connection Error**
```bash
ConnectionError: Could not connect to Ollama
```
*Solution*: Start the Ollama service:
```bash
ollama serve
# In another terminal:
ollama pull llama2  # or your preferred model
```

### Getting Help

- **MTLLM Documentation**: [https://www.jac-lang.org/learn/jac-mtllm/](https://www.jac-lang.org/learn/jac-mtllm/)
- **Jac Language Guide**: [https://www.jac-lang.org](https://www.jac-lang.org)
- **Issues**: Report bugs or ask questions via GitHub Issues